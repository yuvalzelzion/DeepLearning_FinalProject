{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59oRhvZP8-wF"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_PjiJCyKsKx"
      },
      "source": [
        "###Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pFR0G5v8wGh"
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.13.1\n",
        "!pip3 install opencv-python\n",
        "!pip3 install imageai --upgrade\n",
        "!pip3 install tensorflow==2.5.0rc0\n",
        "!pip3 install keras==2.5.0rc0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpGcKjBFL-_I"
      },
      "source": [
        "###Connect to google drive to get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-dvRQiXFVLD"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2US1KVpSFeMn"
      },
      "source": [
        "%cd gdrive/MyDrive/School/DeepLearning/FinalProj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSP8JCA99LYs"
      },
      "source": [
        "!unzip apple_detection_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjN1nbwgMGCr"
      },
      "source": [
        "###Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_fzSop29knb"
      },
      "source": [
        "from imageai.Detection.Custom import DetectionModelTrainer\n",
        "\n",
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"apple_dataset\")\n",
        "trainer.setTrainConfig(object_names_array=[\"apple\", \"damaged_apple\"], batch_size=8, num_experiments=50, train_from_pretrained_model=\"pretrained-yolov3.h5\")\n",
        "trainer.trainModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK07vxLVMI_w"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mvAzDuioJdT"
      },
      "source": [
        "### Run th model on a given image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTnJL7Aq_Pdk"
      },
      "source": [
        "from imageai.Detection.Custom import CustomObjectDetection\n",
        "import os\n",
        "\n",
        "execution_path = os.getcwd()\n",
        "\n",
        "detector = CustomObjectDetection()\n",
        "detector.setModelTypeAsYOLOv3()\n",
        "detector.setModelPath(detection_model_path=\"detection_model-ex-028--loss-8.723.h5\")\n",
        "detector.setJsonPath(configuration_json=\"apple_dataset/json/detection_config.json\")\n",
        "detector.loadModel()\n",
        "\n",
        "detections = detector.detectObjectsFromImage(input_image=\"image2.jpg\", minimum_percentage_probability=60, output_image_path=\"image-new2.jpg\")\n",
        "\n",
        "numOfApple = 0\n",
        "for detection in detections:\n",
        "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n",
        "    numOfApple+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InmG0VyoDuh"
      },
      "source": [
        "### Display the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YztTB3cEOVQq"
      },
      "source": [
        "import cv2 \n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "image = cv2.imread(\"image2-new.jpg\")\n",
        "cv2_imshow(image)\n",
        "print(\"Total number of apples in the image:\", numOfApple)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T94cmIUHM6VO"
      },
      "source": [
        "##Our addition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzpbdw6qog-H"
      },
      "source": [
        "### Crop the apples and save as new images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEOg-DKWOsfl"
      },
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "imageName = \"image2\"\n",
        "im = Image.open(imageName + \".jpg\")\n",
        "i=1\n",
        "for detection in detections:\n",
        "  left=detection[\"box_points\"][0]\n",
        "  top=detection[\"box_points\"][1]\n",
        "  right=detection[\"box_points\"][2]\n",
        "  bottom=detection[\"box_points\"][3]\n",
        "  im1 = im.crop((left, top, right, bottom))\n",
        "  #%matplotlib inline\n",
        "  im1.save(\"cropped/\" + imageName + \"-\" + detection[\"name\"] + str(i) + \".png\")\n",
        "  i+=1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8wCCLfho9RU"
      },
      "source": [
        "### Read each cropped image, convert to black&white accordig to the ripe color, calculate the ripe percentage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysphbPJrq7B0"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(\"cropped\") if isfile(join(\"cropped\", f))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSMq2telNHfz"
      },
      "source": [
        "for f in  onlyfiles: \n",
        "  image = cv2.imread(\"cropped/\" + f)  \n",
        "  hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "  \n",
        "  #define upper and lower color values in HSV.\n",
        "  weaker = np.array([0,0,100])\n",
        "  stronger = np.array([10,255,255])\n",
        "  \n",
        "  mask = cv2.inRange(hsv, weaker, stronger) \n",
        "\n",
        "  #Show original image and result\n",
        "  cv2_imshow(image)\n",
        "  cv2_imshow(mask)\n",
        "  size = np.size(mask)\n",
        "  print(\"Ripe percentage:\", cv2.countNonZero(mask)/size*100*1.5,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}